<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang=""> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8" lang=""> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9" lang=""> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang=""> <!--<![endif]-->
<!-- TODO: bigger "REGISTER" button -->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>WRAI 2017</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="apple-touch-icon" href="apple-touch-icon.png">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 50px;
                padding-bottom: 20px;
            }
	    div#speakers img {
		    width: 70%;
		box-shadow: 1px 1px 5px rgba(0,0,0,0.2), -1px -1px 5px rgba(0,0,0,0.2);
	    }
        </style>
	<!--<link rel="stylesheet" href="css/bootstrap-theme.min.css">-->
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.8.3-respond-1.4.2.min.js"></script>
    </head>
    <body>
        <!--[if lt IE 8]>
            <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
        <![endif]-->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">WRAI 2017</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                        <li>
                                <a href="#contact">Contact</a>
                        </li>
                </ul>
        </div><!--/.navbar-collapse -->
      </div>
    </nav>

    <div class="jumbotron" style="background-image: url('img/eth-uni-banner-3.jpg');
	      background-position: center; background-size: cover; margin-bottom: 0;">
      <div class="container">
        <h1 style="font-size: 280%; color: white; text-shadow: 1px 1px 10px black, 2px 2px 10px black;">
				  Workshop on Reliable Artificial Intelligence 2017
				</h1>
	</div>
    </div>

    <div class="container">
      <!-- Example row of columns -->
      <div class="row">
        <div class="col-md-12">

		<h2>Overview</h2>

	 <p>
           With recent rapid progress in machine learning and artificial intelligence,
           expert attention is increasingly turning to the impact of these fields on society.
	   An awareness is growing of the potential for serious incidents to occur through,
	   for example, design error, badly-specified objectives, or plain misapplication of AI.
	</p>

	<p>
           Multiple institutions both public and private are therefore working on developing
	   improvements to existing AI systems to make them more secure against such failure modes,
	   or on foundational extensions of the field of AI to allow construction of agents which
	   are secure by design. These institutions include <a href="https://openai.com/">OpenAI</a>,
           <a href="https://deepmind.com/">DeepMind</a>, the <a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a>
           at the University of Oxford, and the <a href="https://intelligence.org/">Machine Intelligence Research Institute</a>.
	 </p>

	 <p>Bringing a taste of this research to Switzerland, <b>the Workshop on Reliable Artificial Intelligence was held at ETH Zürich on the 28th of October, 2017.</b> Featuring talks from</p>
		<ul>
			<li>Max Daniel (Effective Altruism Foundation),</li>
			<li>Victoria Krakovna (Google DeepMind),</li>
			<li>Owain Evans (University of Oxford),</li>
			<li>Felix Berkenkamp (ETH Zürich),</li>
			<li>Will Sawin (ETH Zürich) and</li>
			<li>Bas Steunebrunk (NNAISENSE),</li>
		</ul>
		<p>the workshop brought together students and researchers for a day of discussion on technical aspects of building safe artificial intelligence.</p>
		
		<p>
		See below for selected talks and slides.
		</p>

        </div>
      </div>
    </div>

    <div class="container">
      <!-- Example row of columns -->
      <div class="row">
        <div class="col-md-6">
		<img class="img-responsive img-thumbnail" src="photos/IMG_3005.jpg" />
        </div>
        <div class="col-md-6">
		<img class="img-responsive img-thumbnail" src="photos/IMG_3007.jpg" />
        </div>
        <div class="col-md-6">
        </div>
      </div>

      <div class="row">
        <div class="col-md-6">
		<img class="img-responsive img-thumbnail" src="photos/IMG_3017.jpg" />
        </div>
        <div class="col-md-6">
		<img class="img-responsive img-thumbnail" src="photos/IMG_3056.jpg" />
        </div>
        <div class="col-md-6">
        </div>
      </div>

      <div class="row">
        <div class="col-md-6">
		<img class="img-responsive img-thumbnail" src="photos/IMG_3061.jpg" />
        </div>
        <div class="col-md-6">
		<img class="img-responsive img-thumbnail" src="photos/IMG_3075.jpg" />
        </div>
        <div class="col-md-6">
        </div>
      </div>
    </div>


    <div class="container">
      <div class="row">
        <div class="col-md-12">
         <h2>Talks</h2>
	</div>
      </div>
      <!-- Example row of columns -->
      <div class="row">
        <div class="col-md-6">

	<h3>An Overview of the AI Safety Landscape</h3>
	<h4>Max Daniel</h4>

<iframe width="560" height="315" src="https://www.youtube.com/embed/t2yscS5f0q8?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
<br />
<br />


<p>Recent years have seen a surge of interest in exploring the societal consequences of increasingly autonomous and capable artificial intelligence (AI) systems. Next to understanding potential economic and legal challenges associated with AI, a growing body of work in technical AI research and machine learning aims to provide the engineering and design foundations for ensuring that future AI systems will remain safe and beneficial. This talk gives an overview of this thriving field of AI safety, with a focus on the following questions. What are the technical problems addressed by AI safety research, and how do they relate to both short-term and long-term risks and benefits of AI? Who are the key actors funding and conducting AI safety research? How can interested students and researchers get involved in AI safety research?</p>
<p><a href="slides/An Overview of the AI Safety Landscape - Max Daniel.pdf">Slides</a>. See also the <a href="slides/Closing Remarks - Max Daniel.pdf">slides from the closing remarks</a> given by Max Daniel, containing information on groups doing technical AI safety research and other useful resources.</p>
        </div>

        <div class="col-md-6">
<h3>Growing Robust & Safe AI: Let's be Realistic</h3>
<h4>Bas Steunebrink</h4>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UiaPIW7e5a4?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
<br />
<br />

<p>Learning values and ethics under pragmatic real-world constraints requires a new developmental approach to training AIs. This approach places significantly more responsibility on the "teachers" of AI than on the designers and builders. For an AI to move from brittle to robust understanding of what the referents of (possibly evolving) ethics specifications really mean, it needs to properly ground its knowledge in the pragmatics of the world. Therefore the developmental approach entails that we not only figure out which things to teach in which order, but also how to measure progress, such that it becomes feasible to ultimately certify the robustness of the AI to predict ethics violations and report and act accordingly.</p>
<a href="slides/Growing Robust & Safe AI - Let's be Realistic - Bas Steunebrink.pdf">Slides</a>
        </div>

      </div>

      <div class="row">

        <div class="col-md-6">
      <h3>Reinforcement Learning with a Corrupted Reward Channel</h3>
      <h4>Victoria Krakovna</h4>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ay5flNRxtZk?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
<br />
<br />

<p>No real-world reward function is perfect. Reward misspecifications, sensory errors, and software bugs may result in RL agents observing higher (or lower) rewards in some states than they should, which can lead to undesirable or dangerous behavior. We formalize this problem as a generalized Markov Decision Problem, and show that traditional RL methods fare poorly in this setting, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. We develop an abstract framework for giving the agent richer data by cross-checking reward information between different states. This framework encompasses inverse reinforcement learning and semi-supervised reinforcement learning, and helps the agent overcome reward corruption under some assumptions.</p>
<p><a href="slides/Reinforcement Learning with a Corrupted Reward Channel - Victoria Krakovna.pdf">Slides</a>; <a href="https://arxiv.org/pdf/1705.08417.pdf">paper</a></p>
        </div>
        <div class="col-md-6">
	<h3>Safe Reinforcement Learning in Robotics with Bayesian Models</h3>
	<h4>Felix Berkenkamp</h4>
<iframe width="560" height="315" src="https://www.youtube.com/embed/L3G1gJDWYWs?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>

<br />
<br />
<p>Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, most reinforcement learning algorithms rely on random exploration to find optimal policies, which may be harmful in real-world systems such as robots. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this talk, we show how the uncertainty information in Bayesian models can be used to make safe and informed decisions both in policy search and model-based reinforcement learning. Moreover, we show how these algorithms can be applied to physical quadrotor vehicles.</p>
<a href="slides/Safe Reinforcement Learning in Robotics with Bayesian Models - Felix Berkenkamp.pdf">Slides</a>
        </div>

      </div>

        <div class="row">
                <div id="contact" class="col-md-12">
                <h2>Contact</h2>
                <p>
                The Workshop on Reliable Artificial Intelligence was organized by MIRIxZürich. Contact Marko Thiel, Michal Pokorný or Matthew Rahtz at <a href="mailto:wrai2017@gmail.com">wrai2017@gmail.com</a>.
                </p>
		</div>
        </div>
			<!--
          <p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
			-->

      <hr>

      <footer>
	      <center>
		      <a href="https://www.ethz/ch/en.html">
			      <img src="img/eth.png" alt="ETH Zürich logo" style="width: 20%">
		      </a>
		      <a href="http://intelligence.org">
			      <img src="img/miri-full.png" alt="MIRI logo" style="width: 20%; margin: 0px 50px">
		      </a>
		      <a href="https://ea-foundation.org/">
			      <img src="img/eaf.png" alt="Effective Altruism Foundation logo" style="width: 20%">
		      </a>
	      </center>
                <p>
                The Workshop on Reliable Artificial Intelligence was organized by MIRIxZürich,
		supported by <a href="https://www.ethz.ch/en.html">ETH Zürich</a>, and
		sponsored by the <a href="http://intelligence.org">Machine Intelligence Research Institute</a> and the <a href="https://ea-foundation.org">Effective Altruism Foundation.</a>
		</p>
		<p>
		<a href="https://commons.wikimedia.org/wiki/File:ETH_Z%C3%BCrich_im_Abendlicht.jpg">Banner image</a>
		by Wikipedia user <a href="https://commons.wikimedia.org/wiki/User:ETH-Bibliothek">ETH-Bibliothek</a>,
		used under the <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA license</a>.
		</p>
      </footer>
    </div> <!-- /container -->        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="js/vendor/jquery-1.11.2.min.js"><\/script>')</script>

        <script src="js/vendor/bootstrap.min.js"></script>

        <script src="js/main.js"></script>

        <!-- Google Analytics -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-105407154-1', 'auto');
          ga('send', 'pageview');

        </script>
    </body>
</html>
