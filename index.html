<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang=""> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8" lang=""> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9" lang=""> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang=""> <!--<![endif]-->
<!-- TODO: bigger "REGISTER" button -->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title></title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="apple-touch-icon" href="apple-touch-icon.png">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 50px;
                padding-bottom: 20px;
            }
	    div#speakers img {
		    width: 70%;
		box-shadow: 1px 1px 5px rgba(0,0,0,0.2), -1px -1px 5px rgba(0,0,0,0.2);
	    }
        </style>
	<!--<link rel="stylesheet" href="css/bootstrap-theme.min.css">-->
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.8.3-respond-1.4.2.min.js"></script>
    </head>
    <body>
        <!--[if lt IE 8]>
            <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
        <![endif]-->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">WRAI 2017</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                        <li>
                                <a href="https://goo.gl/forms/7Ov1HBIByMlIXIyV2">Register</a>
                        </li>
                        <li>
                                <a href="#contact">Contact</a>
                        </li>
                </ul>
        </div><!--/.navbar-collapse -->
      </div>
    </nav>

    <div class="jumbotron" style="background-image: url('img/eth-uni-banner-3.jpg');
	      background-position: center; background-size: cover; margin-bottom: 0;">
      <div class="container">
        <h1 style="font-size: 280%; color: white; text-shadow: 1px 1px 10px black, 2px 2px 10px black;">
				  Workshop on Reliable Artificial Intelligence 2017
				</h1>
				<h2 style="color: white; font-size: 200%; text-shadow: 1px 1px 10px black;">
				  Saturday 28 October 2017, 9:00 - 18:00, Venue: ETH Zürich
				</h2>
	</div>
    </div>
    <div class="jumbotron">
      <div class="container">
	      <p>
          One-day workshop on technical aspects of building robust and safe agents.
          Topics include safe reinforcement learning, safe control and exploration, value learning,
	  and formal analysis of agent behaviour.
        </p>
        <p style="text-align: center;">
        <a class="btn btn-primary btn-lg" href="https://goo.gl/forms/7Ov1HBIByMlIXIyV2"
                role="button" style="font-size: 140%">Register &raquo;</a>
        </p>
      </div>
    </div>

    <div class="container">
      <!-- Example row of columns -->
      <div class="row">
        <div class="col-md-12">
         <h2>Overview</h2>
	 <p>
           With recent rapid progress in machine learning and artificial intelligence,
           expert attention is increasingly turning to the impact of these fields on society.
	   An awareness is growing of the potential for serious incidents to occur through,
	   for example, design error, badly-specified objectives, or plain misapplication of AI.
	</p>

	<p>
           Multiple institutions both public and private are therefore working on developing
	   improvements to existing AI systems to make them more secure against such failure modes,
	   or on foundational extensions of the field of AI to allow construction of agents which
	   are secure by design. These institutions include <a href="https://openai.com/">OpenAI</a>,
           <a href="https://deepmind.com/">DeepMind</a>, the <a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a>
           at the University of Oxford, and the <a href="https://intelligence.org/">Machine Intelligence Research Institute</a>.
	 </p>
         <p>
	   The Workshop on Reliable Artificial Intelligence 2017 brings a taste of this research to Zürich, Switzerland.
	   The workshop will take place on <b>Saturday the 28th of October 2017</b>, running <b>from 9:00 until 18:00</b>, at
	   the <b>LEE building</b>, <b>room E 101</b>, at <b>ETH Zürich</b>. The day will consist of talks from invited speakers,
	   followed by discussion in the late afternoon.
         </p>
        </div>
      </div>
      <div class="row">
        <div class="col-md-12">
         <h2>Talks</h2>
	</div>
      </div>

		<div class="row">
			<div class="col-md-6">
				<h3>An Overview of the AI Safety Landscape</h3>
				<h4>Max Daniel</h4>
				<p>
Recent years have seen a surge of interest in exploring the societal consequences of increasingly autonomous and capable artificial intelligence (AI) systems. Next to understanding potential economic and legal challenges associated with AI, a growing body of work in technical AI research and machine learning aims to provide the engineering and design foundations for ensuring that future AI systems will remain safe and beneficial. This talk gives an overview of this thriving field of AI safety, with a focus on the following questions. What are the technical problems addressed by AI safety research, and how do they relate to both short-term and long-term risks and benefits of AI? Who are the key actors funding and conducting AI safety research? How can interested students and researchers get involved in AI safety research?
				</p>
			</div>

			<div class="col-md-6">
				<h3>Reinforcement Learning with a Corrupted Reward Channel</h3>
				<h4>Victoria Krakovna</h4>
				<p>
	No real-world reward function is perfect. Reward misspecifications, sensory errors, and software bugs may result in RL agents observing higher (or lower) rewards in some states than they should, which can lead to undesirable or dangerous behavior. We formalize this problem as a generalized Markov Decision Problem, and show that traditional RL methods fare poorly in this setting, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. We develop an abstract framework for giving the agent richer data by cross-checking reward information between different states. This framework encompasses inverse reinforcement learning and semi-supervised reinforcement learning, and helps the agent overcome reward corruption under some assumptions.
				</p>
			</div>
		</div>

		<div class="row">
			<div class="col-md-6">
				<h3>Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</h3>
				<h4>Owain Evans</h4>
				<p>
	AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human "in the loop" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours.
				</p>
			</div>

			<div class="col-md-6">
				<h3>Growing Robust &amp; Safe AI: Let's be Realistic</h3>
				<h4>Bas Steunebrink</h4>
				<p>
	Learning values and ethics under pragmatic real-world constraints requires a new developmental approach to training AIs. This approach places significantly more responsibility on the "teachers" of AI than on the designers and builders. For an AI to move from brittle to robust understanding of what the referents of (possibly evolving) ethics specifications really mean, it needs to properly ground its knowledge in the pragmatics of the world. Therefore the developmental approach entails that we not only figure out which things to teach in which order, but also how to measure progress, such that it becomes feasible to ultimately certify the robustness of the AI to predict ethics violations and report and act accordingly.
				</p>
			</div>
		</div>

		<div class="row">
			<div class="col-md-6">
				<h3>Models of Program Equilibrium</h3>
				<h4>Will Sawin</h4>
				<p>
	The field of program equilibrium studies what happens when we design an AI which must choose to cooperate or compete with another AI with independent goals, where both AIs can see each other's source code and thus predict each other's behavior. New strategies and equilibria occur, and it is not at all clear what the best strategy is. We discuss the already nontrivial case where each AI's behavior in a game only depends on the other AI's behavior when playing against simpler programs of the same type.
				</p>
			</div>

			<div class="col-md-6">
				<h3>Safe Reinforcement Learning in Robotics with Bayesian Models</h3>
				<h4>Felix Berkenkamp</h4>
				<p>
	Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, most reinforcement learning algorithms rely on random exploration to find optimal policies, which may be harmful in real-world systems such as robots. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this talk, we show how the uncertainty information in Bayesian models can be used to make safe and informed decisions both in policy search and model-based reinforcement learning. Moreover, we show how these algorithms can be applied to physical quadrotor vehicles.
				</p>
			</div>
		</div>

        <div class="row">
                <div class="col-md-12">
                  <h2>Speakers</h2>
                </div>
	</div>
	<div id="speakers">
	<div class="row">
		<div class="col-md-4">
			<img src="img/headshots/owain-evans-github-io-2.jpg">
			<h3>Owain Evans</h3>
			<h4>Alexander Tamas Postdoc in Artificial Intelligence, University of Oxford</h4>
			<p>Following a PhD from MIT in cognitive science, probabilistic programming and philosophy of science, Owain currently works on AI safety and reinforcement learning at the University of Oxford, where he leads the ‘Inferring Human Preferences’ project.</p>
		</div>
		<div class="col-md-4">
			<img src="img/headshots/victoria-krakovna-site.png" style="width: 70%">
			<h3>Victoria Krakovna</h3>
			<h4>Research scientist, DeepMind</h4>
			<p>Victoria received her PhD in statistics and machine learning from Harvard University, with a thesis focused on interpretable models. She currently works on AI safety at DeepMind.</p>
		</div>
		<div class="col-md-4">
			<img src="img/headshots/felix-bernenkamp-site.jpg">
			<h3>Felix Berkenkamp</h3>
			<h4>PhD student, Learning and Adaptive Systems Group, ETH Zürich</h4>
			<p>Having completed an MSc in Mechanical Engineering at ETH Zürich with thesis research at the University of Toronto, Felix's doctoral research focuses on safe exploration in robotics.</p>
		</div>
	</div>
	<div class="row">
		<div class="col-md-4">
			<img src="img/headshots/will-sawin-2.jpg">
			<h3>Will Sawin</h3>
			<h4>Junior Fellow, Institute for Theoretical Studies, ETH Zürich</h4>
			<p>Will completed his PhD in Mathematics at Princeton University and is now a Junior Fellow at ETH-ITS. His main field of study is arithmetic algebraic geometry, but he has also made significant contributions to combinatorics, category theory and intelligent agent foundations.</p>
		</div>
		<div class="col-md-4">
			<img src="img/headshots/bas-steunebrink.png">
			<h3>Bas Steunebrink</h3>
			<h4>Co-founder, <a href="https://nnaisense.com/">NNAISENSE</a></h4>
			<p>Bas Steunebrink is a co-founder of NNAISENSE, a startup dedicated to building and marketing general-purpose AI. His research interests include artificial general intelligence, machine learning, bounded rationality, and AI safety &amp; ethics. Steunebrink earned his PhD in Artificial Intelligence at Utrecht University in 2010 after which he worked as a postdoc at the Swiss AI lab IDSIA.</p>
		</div>
		<div class="col-md-4">
			<img src="img/headshots/max-daniel.jpg">
			<h3>Max Daniel</h3>
			<h4>Researcher, Effective Altruism Foundation</h4>
			<p>Max works on the ethics of AI at the Effective Altruism Foundation, following a master's in mathematics form Heidelberg University. His research concerns both the short-term and long-term potential issues related to AI.</p>
		</div>
	</div>
	</div>
        <div class="row">
                <div class="col-md-12">
                <h2>Contact</h2>
                <p>
                The Workshop on Reliable Artificial Intelligence is organized by MIRIxZürich.
                You can contact us at <a href="mailto:wrai-workshop@ethz.ch">wrai-workshop@ethz.ch</a>.
                </p>
		</div>
        </div>
			<!--
          <p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
			-->

      <hr>

      <footer>
	      <center>
		      <a href="https://www.ethz/ch/en.html">
			      <img src="img/eth.png" alt="ETH Zürich logo" style="width: 25%">
		      </a>
		      <a href="http://intelligence.org">
			      <img src="img/miri-full.png" alt="MIRI logo" style="width: 25%">
		      </a>
	      </center>
                <p>
                The Workshop on Reliable Artificial Intelligence is organized by MIRIxZürich,
		supported by <a href="https://www.ethz.ch/en.html">ETH Zürich</a> and
		sponsored by the <a href="http://intelligence.org">Machine Intelligence Research Institute</a>.
		</p>
                <p>
                Contact: <a href="mailto:wrai-workshop@ethz.ch">wrai-workshop@ethz.ch</a>
                </p>
		<p>
		<a href="https://commons.wikimedia.org/wiki/File:ETH_Z%C3%BCrich_im_Abendlicht.jpg">Banner image</a>
		by Wikipedia user <a href="https://commons.wikimedia.org/wiki/User:ETH-Bibliothek">ETH-Bibliothek</a>,
		used under the <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA license</a>.
		</p>
      </footer>
    </div> <!-- /container -->        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="js/vendor/jquery-1.11.2.min.js"><\/script>')</script>

        <script src="js/vendor/bootstrap.min.js"></script>

        <script src="js/main.js"></script>

        <!-- Google Analytics -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-105407154-1', 'auto');
          ga('send', 'pageview');

        </script>
    </body>
</html>
